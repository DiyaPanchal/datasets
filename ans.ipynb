{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/mnt/data/HousingData.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocessing\n",
    "# Handle missing values\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=[\"target\"], errors='ignore')  # Replace \"target\" with the actual target column name if known\n",
    "y = data[\"target\"] if \"target\" in data.columns else data.iloc[:, -1]  # Default to the last column as target\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_transformer, numerical_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Base models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", model)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    results[name] = {\n",
    "        \"r2_score\": r2_score(y_test, y_pred),\n",
    "        \"mean_squared_error\": mean_squared_error(y_test, y_pred),\n",
    "        \"model\": pipeline,\n",
    "    }\n",
    "\n",
    "# Feature selection methods\n",
    "feature_selection_methods = {\n",
    "    \"SelectKBest (f_regression)\": SelectKBest(score_func=f_regression, k=5),\n",
    "    \"SelectKBest (mutual_info)\": SelectKBest(score_func=mutual_info_regression, k=5),\n",
    "    \"Random Forest Importance\": RandomForestRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "# Apply feature selection and evaluate models\n",
    "selected_features_results = {}\n",
    "for method_name, selector in feature_selection_methods.items():\n",
    "    if isinstance(selector, SelectKBest):\n",
    "        X_selected = selector.fit_transform(X, y)\n",
    "    else:  # Random Forest-based selection\n",
    "        selector.fit(X, y)\n",
    "        importances = selector.feature_importances_\n",
    "        top_features = np.argsort(importances)[-5:]\n",
    "        X_selected = X.iloc[:, top_features]\n",
    "    \n",
    "    # Train/test split for selected features\n",
    "    X_train_sel, X_test_sel, y_train_sel, y_test_sel = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train_sel, y_train_sel)\n",
    "        y_pred_sel = model.predict(X_test_sel)\n",
    "        selected_features_results[(method_name, model_name)] = {\n",
    "            \"r2_score\": r2_score(y_test_sel, y_pred_sel),\n",
    "            \"mean_squared_error\": mean_squared_error(y_test_sel, y_pred_sel),\n",
    "        }\n",
    "\n",
    "# Compare results\n",
    "print(\"Base Model Results:\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name}: R2={metrics['r2_score']:.4f}, MSE={metrics['mean_squared_error']:.4f}\")\n",
    "\n",
    "print(\"\\nFeature Selection Results:\")\n",
    "for (method, model_name), metrics in selected_features_results.items():\n",
    "    print(f\"{method} + {model_name}: R2={metrics['r2_score']:.4f}, MSE={metrics['mean_squared_error']:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].bar(results.keys(), [metrics[\"r2_score\"] for metrics in results.values()], color=\"blue\")\n",
    "ax[0].set_title(\"R2 Scores (Base Models)\")\n",
    "ax[0].set_ylabel(\"R2 Score\")\n",
    "ax[0].set_xticks(range(len(results)))\n",
    "ax[0].set_xticklabels(results.keys(), rotation=45)\n",
    "\n",
    "ax[1].bar(results.keys(), [metrics[\"mean_squared_error\"] for metrics in results.values()], color=\"orange\")\n",
    "ax[1].set_title(\"Mean Squared Error (Base Models)\")\n",
    "ax[1].set_ylabel(\"MSE\")\n",
    "ax[1].set_xticks(range(len(results)))\n",
    "ax[1].set_xticklabels(results.keys(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
