Step 1: Load and Inspect the Dataset
Goal: Read the dataset into memory and inspect its structure.
Action:
        Use pandas to load the CSV file.
        Examine the dataset for missing values and column types using .info() and .head().


Step 2: Handle Missing Values
Goal: Fill in missing data to make the dataset usable for machine learning models.
Action:
    Use SimpleImputer to replace missing values with the mean of each column.
    Why: Missing values can cause errors or biases during training, so imputing ensures completeness.


Step 3: Normalize/Standardize Features
Goal: Ensure all features are on the same scale to avoid domination of certain features during training.
Action:
    Use StandardScaler to standardize each feature to have a mean of 0 and a standard deviation of 1.
    Why: Models like Linear Regression are sensitive to the scale of input features.


Step 4: Split the Dataset
Goal: Separate data into training and testing sets to evaluate model performance on unseen data.
Action:
    Use train_test_split to divide data into 80% training and 20% testing.
    Why: To prevent overfitting and ensure the model generalizes well.


Step 5: Train Base Models
Goal: Build baseline models for comparison.
Action:
Train two models:
    Linear Regression: Assumes a linear relationship between features and the target.
    Decision Tree Regressor: A tree-based model that can capture non-linear relationships.
Evaluate models using:
    Mean Squared Error (MSE): Measures the average squared difference between predictions and actual values (lower is better).
    R² Score: Represents how well the model explains variance in the target (closer to 1 is better).



Step 6: Apply Feature Selection Techniques
Goal: Reduce the number of features while retaining important information.
Techniques Used:
SelectKBest (f_regression):
    Selects the top k features with the highest correlation with the target using ANOVA F-statistics.
SelectKBest (mutual_info_regression):
    Selects the top k features based on mutual information, which measures dependencies between features and the target.
Principal Component Analysis (PCA):
    Reduces dimensionality by transforming features into a smaller number of principal components while retaining maximum variance.

Why: Feature selection improves computational efficiency and can enhance model interpretability and performance.   


Step 7: Evaluate Models After Feature Selection
Goal: Assess the impact of feature selection on model performance.
Action:
    Retrain the models on the transformed datasets from each feature selection technique.
    Compute MSE and R² for each approach.
Why: To determine whether reducing features improves or degrades model accuracy.



Step 8: Compare Results
Goal: Analyze the effectiveness of each feature selection technique.
Action:
    Aggregate performance metrics into a summary table.
    Plot results using bar charts to visually compare MSE across models and techniques.
Why: Visualization helps identify the most efficient model and feature selection strategy.
